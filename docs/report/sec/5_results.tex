\section{Results and Discussion}
\label{sec:results}

% Present the results for each phase, referring back to the methodology and experiments. Use tables and figures extensively.

\subsection{Clustering and Stage Definition Results}
\begin{itemize}
	\item Show visualizations (PCA/t-SNE plots like \cref{fig:cluster_pca_tsne}) of the final clusters.
	\item Show sensor trend plots (like \cref{fig:sensor_trends_per_cluster}) justifying the mapping from clusters to Stages 0-4. Describe the characteristics of each stage based on sensor behavior.
	\item Discuss any challenges during interpretation (e.g., fuzzy boundaries between clusters).
	\item Compare KMeans vs. Agglomerative results if both were seriously considered. Justify the final choice.
	\item Report final distribution of data points across the 5 stages (highlighting imbalance). (Placeholder for \cref{tab:stage_distribution}).
\end{itemize}

% --- Table Placeholder ---
% \begin{table}[t]
%   \centering
%   \caption{Distribution of Data Points Across Defined Degradation Stages (FD001 - Training Set)}
%   \label{tab:stage_distribution}
%   \begin{tabular}{@{}lccccc@{}}
%     \toprule
%     Stage & 0 (Normal) & 1 (Slight) & 2 (Moderate) & 3 (Critical) & 4 (Failure) \\
%     \midrule
%     Count & XXXX & XXXX & XXXX & XXX & XXX \\
%     Percent & XX.X\% & XX.X\% & XX.X\% & X.X\% & X.X\% \\
%     \bottomrule
%   \end{tabular}
% \end{table}
% ------------------------

\subsection{Classification Performance}
\begin{itemize}
	\item Present results in tables. Compare different models (Baseline, RF, XGBoost) and impact of imbalance handling. (Placeholder for \cref{tab:classification_results}).
	\item Show the confusion matrix for the best model (\cref{fig:confusion_matrix}). Discuss common misclassifications (e.g., Stage 2 vs 3).
	\item Analyze per-class performance – how well are the critical/failure stages (3, 4) identified?
	\item Discuss feature importance results (\cref{fig:feature_importance_cls}). Which sensors/features are most indicative of different stages?
\end{itemize}

% --- Table Placeholder ---
% \begin{table*}[t]
%   \centering
%   \caption{Classification Performance Comparison on Validation Set (FD001)}
%   \label{tab:classification_results}
%   \resizebox{\textwidth}{!}{% Resize table to fit page width if needed
%   \begin{tabular}{@{}l|ccc|ccc|ccc|ccc|ccc@{}}
%     \toprule
%     \multicolumn{1}{c|}{\multirow{2}{*}{Model}} & \multicolumn{3}{c|}{Stage 0} & \multicolumn{3}{c|}{Stage 1} & \multicolumn{3}{c|}{Stage 2} & \multicolumn{3}{c|}{Stage 3} & \multicolumn{3}{c}{Stage 4} \\
%      & P & R & F1 & P & R & F1 & P & R & F1 & P & R & F1 & P & R & F1 & Macro F1 & Weighted F1 \\
%     \midrule
%     Logistic Regression & . & . & . & . & . & . & . & . & . & . & . & . & . & . & . & . & . \\
%     Random Forest (Balanced) & . & . & . & . & . & . & . & . & . & . & . & . & . & . & . & . & . \\
%     XGBoost (Balanced) & . & . & . & . & . & . & . & . & . & . & . & . & . & . & . & . & . \\
%     % Add other models/variants as needed
%     \bottomrule
%   \end{tabular}
%   } % end resizebox
%   \small P: Precision, R: Recall, F1: F1-Score. Best results per metric highlighted.
% \end{table*}
% ------------------------

\subsection{Regression Performance}
\begin{itemize}
	\item Present RMSE, MAE, R² results in a table, comparing different regression models. (Placeholder for \cref{tab:regression_results}).
	\item Discuss the performance – how accurately can the time-to-next-stage/failure be predicted?
	\item Analyze error patterns if possible (e.g., does error increase closer to failure?).
	\item Compare performance across different stages if applicable.
\end{itemize}

% --- Table Placeholder ---
% \begin{table}[t]
%   \centering
%   \caption{Regression Performance Comparison (Time-to-Next-Stage/Failure) on Validation Set (FD001)}
%   \label{tab:regression_results}
%   \begin{tabular}{@{}lccc@{}}
%     \toprule
%     Model & RMSE & MAE & R² Score \\
%     \midrule
%     Linear Regression (Ridge) & . & . & . \\
%     Random Forest Regressor & . & . & . \\
%     XGBoost Regressor & . & . & . \\
%     % Add other models as needed
%     \bottomrule
%   \end{tabular}
% \end{table}
% ------------------------

\subsection{Risk Score Analysis}
\begin{itemize}
	\item Show example plots of Risk Score evolution over time (\cref{fig:risk_score_trend}). Discuss how the score reflects degradation progression.
	\item Explain the chosen alert threshold ($\theta$) and the rationale (e.g., based on PR curve analysis). Show the PR curve if informative. (Placeholder for \cref{fig:pr_curve_risk}).
	\item Discuss the effectiveness of the Risk Score in providing timely warnings compared to just looking at classification or regression alone.
	\item Analyze potential failure modes or limitations of the Risk Score.
\end{itemize}

% --- Figure Placeholder ---
% \begin{figure}[t]
%   \centering
%   \includegraphics[width=0.7\linewidth]{figures/pr_curve_risk.pdf} % Replace with your PR curve
%   \caption{Precision-Recall curve for Risk Score threshold tuning on the validation set. The chosen threshold $\theta$ is marked.}
%   \label{fig:pr_curve_risk}
% \end{figure}
% --------------------------

\subsection{Overall Discussion}
\begin{itemize}
	\item Synthesize the results from all phases. Does the hybrid approach provide a more comprehensive view of engine health?
	\item Discuss the benefits and drawbacks of using data-driven stages vs. standard RUL.
	\item Limitations of the study (e.g., based on simulation data, assumptions made, sensitivity to clustering).
	\item Potential impact and applicability in real-world scenarios.
\end{itemize}

% ======================================================================
